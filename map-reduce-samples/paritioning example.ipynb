{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir data\n",
    "!echo \"apple map data google monitor ninja basket bag vahid view water were X Xray Yellow zebra\" > data/text1.txt\n",
    "!echo \"cat dog horse snake lunch food game I jacket key keyboard kinght Jay Lemon light land\" > data/another_file.txt\n",
    "!echo \"noon open order pie pick push quite quick rest rapid range rain sample simple tea tiger umberella upload\" > data/yet_another_file.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "put: `/inputs/partition/data/another_file.txt': File exists\r\n",
      "put: `/inputs/partition/data/yet_another_file.txt': File exists\r\n",
      "put: `/inputs/partition/data/text1.txt': File exists\r\n"
     ]
    }
   ],
   "source": [
    "!hadoop fs -mkdir -p /outputs/partition\n",
    "!hadoop fs -mkdir -p /inputs/partition\n",
    "!hadoop fs -put data /inputs/partition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: Use of this script to execute dfs is deprecated.\n",
      "WARNING: Attempting to execute replacement \"hdfs dfs\" instead.\n",
      "\n",
      "Found 3 items\n",
      "-rw-r--r--   3 root supergroup         86 2020-10-14 19:57 /inputs/partition/data/another_file.txt\n",
      "-rw-r--r--   3 root supergroup         89 2020-10-14 19:57 /inputs/partition/data/text1.txt\n",
      "-rw-r--r--   3 root supergroup        105 2020-10-14 19:57 /inputs/partition/data/yet_another_file.txt\n"
     ]
    }
   ],
   "source": [
    "!hadoop dfs -ls /inputs/partition/data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sort example with only one reducer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mapper.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mapper.py\n",
    "#!/opt/bitnami/python/bin/python\n",
    "# -*-coding:utf-8 -*\n",
    "import sys\n",
    "\n",
    "for line in sys.stdin:\n",
    "    line = line.strip()\n",
    "    words = line.split()\n",
    "\n",
    "    for word in words: # writes to stdout\n",
    "        word = word.lower()\n",
    "        print(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting reducer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile reducer.py\n",
    "#!/opt/bitnami/python/bin/python\n",
    "# -*-coding:utf-8 -*\n",
    "\n",
    "import sys\n",
    "total = 0\n",
    "\n",
    "for line in sys.stdin:\n",
    "    line = line.strip()\n",
    "    print(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-10-14 20:07:34,319 WARN streaming.StreamJob: -file option is deprecated, please use generic option -files instead.\n",
      "packageJobJar: [/training/paritioning/mapper.py, /training/paritioning/reducer.py, /tmp/hadoop-unjar3512409146229778532/] [] /tmp/streamjob4423979721193721102.jar tmpDir=null\n",
      "2020-10-14 20:07:34,955 INFO client.RMProxy: Connecting to ResourceManager at resourcemanager/172.18.0.2:8032\n",
      "2020-10-14 20:07:35,086 INFO client.AHSProxy: Connecting to Application History server at historyserver/172.18.0.6:10200\n",
      "2020-10-14 20:07:35,183 INFO client.RMProxy: Connecting to ResourceManager at resourcemanager/172.18.0.2:8032\n",
      "2020-10-14 20:07:35,183 INFO client.AHSProxy: Connecting to Application History server at historyserver/172.18.0.6:10200\n",
      "2020-10-14 20:07:35,363 INFO mapreduce.JobResourceUploader: Disabling Erasure Coding for path: /tmp/hadoop-yarn/staging/root/.staging/job_1602682693586_0006\n",
      "2020-10-14 20:07:35,439 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false\n",
      "2020-10-14 20:07:35,499 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false\n",
      "2020-10-14 20:07:35,547 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false\n",
      "2020-10-14 20:07:36,078 INFO mapred.FileInputFormat: Total input files to process : 3\n",
      "2020-10-14 20:07:36,092 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false\n",
      "2020-10-14 20:07:36,103 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false\n",
      "2020-10-14 20:07:36,108 INFO mapreduce.JobSubmitter: number of splits:3\n",
      "2020-10-14 20:07:36,179 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false\n",
      "2020-10-14 20:07:36,192 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1602682693586_0006\n",
      "2020-10-14 20:07:36,192 INFO mapreduce.JobSubmitter: Executing with tokens: []\n",
      "2020-10-14 20:07:36,309 INFO conf.Configuration: resource-types.xml not found\n",
      "2020-10-14 20:07:36,309 INFO resource.ResourceUtils: Unable to find 'resource-types.xml'.\n",
      "2020-10-14 20:07:36,565 INFO impl.YarnClientImpl: Submitted application application_1602682693586_0006\n",
      "2020-10-14 20:07:36,624 INFO mapreduce.Job: The url to track the job: http://resourcemanager:8088/proxy/application_1602682693586_0006/\n",
      "2020-10-14 20:07:36,625 INFO mapreduce.Job: Running job: job_1602682693586_0006\n",
      "2020-10-14 20:07:40,701 INFO mapreduce.Job: Job job_1602682693586_0006 running in uber mode : false\n",
      "2020-10-14 20:07:40,707 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "2020-10-14 20:07:45,797 INFO mapreduce.Job:  map 33% reduce 0%\n",
      "2020-10-14 20:07:46,803 INFO mapreduce.Job:  map 67% reduce 0%\n",
      "2020-10-14 20:07:47,822 INFO mapreduce.Job:  map 100% reduce 0%\n",
      "2020-10-14 20:07:49,830 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "2020-10-14 20:07:49,844 INFO mapreduce.Job: Job job_1602682693586_0006 completed successfully\n",
      "2020-10-14 20:07:49,915 INFO mapreduce.Job: Counters: 54\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=259\n",
      "\t\tFILE: Number of bytes written=931910\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=610\n",
      "\t\tHDFS: Number of bytes written=330\n",
      "\t\tHDFS: Number of read operations=14\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=2\n",
      "\t\tHDFS: Number of bytes read erasure-coded=0\n",
      "\tJob Counters \n",
      "\t\tLaunched map tasks=3\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tRack-local map tasks=3\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=17948\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=10944\n",
      "\t\tTotal time spent by all map tasks (ms)=4487\n",
      "\t\tTotal time spent by all reduce tasks (ms)=1368\n",
      "\t\tTotal vcore-milliseconds taken by all map tasks=4487\n",
      "\t\tTotal vcore-milliseconds taken by all reduce tasks=1368\n",
      "\t\tTotal megabyte-milliseconds taken by all map tasks=18378752\n",
      "\t\tTotal megabyte-milliseconds taken by all reduce tasks=11206656\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=3\n",
      "\t\tMap output records=50\n",
      "\t\tMap output bytes=330\n",
      "\t\tMap output materialized bytes=334\n",
      "\t\tInput split bytes=330\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=50\n",
      "\t\tReduce shuffle bytes=334\n",
      "\t\tReduce input records=50\n",
      "\t\tReduce output records=50\n",
      "\t\tSpilled Records=100\n",
      "\t\tShuffled Maps =3\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=3\n",
      "\t\tGC time elapsed (ms)=142\n",
      "\t\tCPU time spent (ms)=1350\n",
      "\t\tPhysical memory (bytes) snapshot=1078878208\n",
      "\t\tVirtual memory (bytes) snapshot=23498461184\n",
      "\t\tTotal committed heap usage (bytes)=1152385024\n",
      "\t\tPeak Map Physical memory (bytes)=301039616\n",
      "\t\tPeak Map Virtual memory (bytes)=5039366144\n",
      "\t\tPeak Reduce Physical memory (bytes)=189894656\n",
      "\t\tPeak Reduce Virtual memory (bytes)=8383909888\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=280\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=330\n",
      "2020-10-14 20:07:49,915 INFO streaming.StreamJob: Output directory: /outputs/partition/sample5\n"
     ]
    }
   ],
   "source": [
    "!hadoop jar /opt/hadoop-3.2.1/share/hadoop/tools/lib/hadoop-streaming-3.2.1.jar \\\n",
    "    -file $PWD/mapper.py\\\n",
    "    -file $PWD/reducer.py\\\n",
    "    -mapper mapper.py \\\n",
    "    -reducer reducer.py \\\n",
    "    -input /inputs/partition/data \\\n",
    "    -output /outputs/partition/sample5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2 items\r\n",
      "-rw-r--r--   3 root supergroup          0 2020-10-14 20:07 /outputs/partition/sample5/_SUCCESS\r\n",
      "-rw-r--r--   3 root supergroup        330 2020-10-14 20:07 /outputs/partition/sample5/part-00000\r\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -ls /outputs/partition/sample5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-10-14 20:08:04,635 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false\n",
      "apple\t\n",
      "bag\t\n",
      "basket\t\n",
      "cat\t\n",
      "data\t\n",
      "dog\t\n",
      "food\t\n",
      "game\t\n",
      "google\t\n",
      "horse\t\n",
      "i\t\n",
      "jacket\t\n",
      "jay\t\n",
      "key\t\n",
      "keyboard\t\n",
      "kinght\t\n",
      "land\t\n",
      "lemon\t\n",
      "light\t\n",
      "lunch\t\n",
      "map\t\n",
      "monitor\t\n",
      "ninja\t\n",
      "noon\t\n",
      "open\t\n",
      "order\t\n",
      "pick\t\n",
      "pie\t\n",
      "push\t\n",
      "quick\t\n",
      "quite\t\n",
      "rain\t\n",
      "range\t\n",
      "rapid\t\n",
      "rest\t\n",
      "sample\t\n",
      "simple\t\n",
      "snake\t\n",
      "tea\t\n",
      "tiger\t\n",
      "umberella\t\n",
      "upload\t\n",
      "vahid\t\n",
      "view\t\n",
      "water\t\n",
      "were\t\n",
      "x\t\n",
      "xray\t\n",
      "yellow\t\n",
      "zebra\t\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -cat /outputs/partition/sample5/part-00000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Same example with multiple reducers (this implementation doesn't get the full mark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-10-14 20:09:42,143 WARN streaming.StreamJob: -file option is deprecated, please use generic option -files instead.\n",
      "packageJobJar: [/training/paritioning/mapper.py, /training/paritioning/reducer.py, /tmp/hadoop-unjar1665217619277814910/] [] /tmp/streamjob4192450365862713034.jar tmpDir=null\n",
      "2020-10-14 20:09:42,786 INFO client.RMProxy: Connecting to ResourceManager at resourcemanager/172.18.0.2:8032\n",
      "2020-10-14 20:09:42,906 INFO client.AHSProxy: Connecting to Application History server at historyserver/172.18.0.6:10200\n",
      "2020-10-14 20:09:42,926 INFO client.RMProxy: Connecting to ResourceManager at resourcemanager/172.18.0.2:8032\n",
      "2020-10-14 20:09:42,926 INFO client.AHSProxy: Connecting to Application History server at historyserver/172.18.0.6:10200\n",
      "2020-10-14 20:09:43,044 INFO mapreduce.JobResourceUploader: Disabling Erasure Coding for path: /tmp/hadoop-yarn/staging/root/.staging/job_1602682693586_0007\n",
      "2020-10-14 20:09:43,125 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false\n",
      "2020-10-14 20:09:43,187 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false\n",
      "2020-10-14 20:09:43,205 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false\n",
      "2020-10-14 20:09:43,253 INFO mapred.FileInputFormat: Total input files to process : 3\n",
      "2020-10-14 20:09:43,280 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false\n",
      "2020-10-14 20:09:43,298 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false\n",
      "2020-10-14 20:09:43,317 INFO mapreduce.JobSubmitter: number of splits:3\n",
      "2020-10-14 20:09:43,342 INFO Configuration.deprecation: mapred.reduce.tasks is deprecated. Instead, use mapreduce.job.reduces\n",
      "2020-10-14 20:09:43,396 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false\n",
      "2020-10-14 20:09:43,422 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1602682693586_0007\n",
      "2020-10-14 20:09:43,422 INFO mapreduce.JobSubmitter: Executing with tokens: []\n",
      "2020-10-14 20:09:43,555 INFO conf.Configuration: resource-types.xml not found\n",
      "2020-10-14 20:09:43,555 INFO resource.ResourceUtils: Unable to find 'resource-types.xml'.\n",
      "2020-10-14 20:09:43,810 INFO impl.YarnClientImpl: Submitted application application_1602682693586_0007\n",
      "2020-10-14 20:09:43,921 INFO mapreduce.Job: The url to track the job: http://resourcemanager:8088/proxy/application_1602682693586_0007/\n",
      "2020-10-14 20:09:43,925 INFO mapreduce.Job: Running job: job_1602682693586_0007\n",
      "2020-10-14 20:09:49,012 INFO mapreduce.Job: Job job_1602682693586_0007 running in uber mode : false\n",
      "2020-10-14 20:09:49,016 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "2020-10-14 20:09:54,075 INFO mapreduce.Job:  map 67% reduce 0%\n",
      "2020-10-14 20:09:55,087 INFO mapreduce.Job:  map 100% reduce 0%\n",
      "2020-10-14 20:09:57,105 INFO mapreduce.Job:  map 100% reduce 25%\n",
      "2020-10-14 20:09:59,125 INFO mapreduce.Job:  map 100% reduce 50%\n",
      "2020-10-14 20:10:01,133 INFO mapreduce.Job:  map 100% reduce 75%\n",
      "2020-10-14 20:10:05,153 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "2020-10-14 20:10:06,189 INFO mapreduce.Job: Job job_1602682693586_0007 completed successfully\n",
      "2020-10-14 20:10:06,294 INFO mapreduce.Job: Counters: 54\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=407\n",
      "\t\tFILE: Number of bytes written=1630875\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=610\n",
      "\t\tHDFS: Number of bytes written=330\n",
      "\t\tHDFS: Number of read operations=29\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=8\n",
      "\t\tHDFS: Number of bytes read erasure-coded=0\n",
      "\tJob Counters \n",
      "\t\tLaunched map tasks=3\n",
      "\t\tLaunched reduce tasks=4\n",
      "\t\tRack-local map tasks=3\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=19284\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=45456\n",
      "\t\tTotal time spent by all map tasks (ms)=4821\n",
      "\t\tTotal time spent by all reduce tasks (ms)=5682\n",
      "\t\tTotal vcore-milliseconds taken by all map tasks=4821\n",
      "\t\tTotal vcore-milliseconds taken by all reduce tasks=5682\n",
      "\t\tTotal megabyte-milliseconds taken by all map tasks=19746816\n",
      "\t\tTotal megabyte-milliseconds taken by all reduce tasks=46546944\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=3\n",
      "\t\tMap output records=50\n",
      "\t\tMap output bytes=330\n",
      "\t\tMap output materialized bytes=571\n",
      "\t\tInput split bytes=330\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=50\n",
      "\t\tReduce shuffle bytes=571\n",
      "\t\tReduce input records=50\n",
      "\t\tReduce output records=50\n",
      "\t\tSpilled Records=100\n",
      "\t\tShuffled Maps =12\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=12\n",
      "\t\tGC time elapsed (ms)=225\n",
      "\t\tCPU time spent (ms)=2510\n",
      "\t\tPhysical memory (bytes) snapshot=1646313472\n",
      "\t\tVirtual memory (bytes) snapshot=48648208384\n",
      "\t\tTotal committed heap usage (bytes)=1779957760\n",
      "\t\tPeak Map Physical memory (bytes)=296247296\n",
      "\t\tPeak Map Virtual memory (bytes)=5038931968\n",
      "\t\tPeak Reduce Physical memory (bytes)=191410176\n",
      "\t\tPeak Reduce Virtual memory (bytes)=8386105344\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=280\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=330\n",
      "2020-10-14 20:10:06,294 INFO streaming.StreamJob: Output directory: /outputs/partition/sample6\n"
     ]
    }
   ],
   "source": [
    "!hadoop jar /opt/hadoop-3.2.1/share/hadoop/tools/lib/hadoop-streaming-3.2.1.jar \\\n",
    "    -D mapred.reduce.tasks=4 \\\n",
    "    -file $PWD/mapper.py\\\n",
    "    -file $PWD/reducer.py\\\n",
    "    -mapper mapper.py \\\n",
    "    -reducer reducer.py \\\n",
    "    -input /inputs/partition/data \\\n",
    "    -output /outputs/partition/sample6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 5 items\r\n",
      "-rw-r--r--   3 root supergroup          0 2020-10-14 20:10 /outputs/partition/sample6/_SUCCESS\r\n",
      "-rw-r--r--   3 root supergroup         48 2020-10-14 20:09 /outputs/partition/sample6/part-00000\r\n",
      "-rw-r--r--   3 root supergroup         94 2020-10-14 20:09 /outputs/partition/sample6/part-00001\r\n",
      "-rw-r--r--   3 root supergroup         71 2020-10-14 20:09 /outputs/partition/sample6/part-00002\r\n",
      "-rw-r--r--   3 root supergroup        117 2020-10-14 20:10 /outputs/partition/sample6/part-00003\r\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -ls /outputs/partition/sample6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-10-14 20:10:34,143 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false\n",
      "horse\t\n",
      "i\t\n",
      "keyboard\t\n",
      "kinght\t\n",
      "land\t\n",
      "quick\t\n",
      "range\t\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -cat /outputs/partition/sample6/part-00000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-10-14 20:10:36,811 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false\n",
      "apple\t\n",
      "cat\t\n",
      "jay\t\n",
      "light\t\n",
      "lunch\t\n",
      "monitor\t\n",
      "ninja\t\n",
      "noon\t\n",
      "order\t\n",
      "quite\t\n",
      "rain\t\n",
      "rest\t\n",
      "yellow\t\n",
      "zebra\t\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -cat /outputs/partition/sample6/part-00001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-10-14 20:10:38,773 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false\r\n",
      "google\t\r\n",
      "key\t\r\n",
      "lemon\t\r\n",
      "pick\t\r\n",
      "tiger\t\r\n",
      "umberella\t\r\n",
      "upload\t\r\n",
      "view\t\r\n",
      "water\t\r\n",
      "were\t\r\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -cat /outputs/partition/sample6/part-00002"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-10-14 20:10:43,984 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false\n",
      "bag\t\n",
      "basket\t\n",
      "data\t\n",
      "dog\t\n",
      "food\t\n",
      "game\t\n",
      "jacket\t\n",
      "map\t\n",
      "open\t\n",
      "pie\t\n",
      "push\t\n",
      "rapid\t\n",
      "sample\t\n",
      "simple\t\n",
      "snake\t\n",
      "tea\t\n",
      "vahid\t\n",
      "x\t\n",
      "xray\t\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -cat /outputs/partition/sample6/part-00003"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Partitioning Example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hadoop has a library class, KeyFieldBasedPartitioner, that is useful for many applications. This class allows the Map/Reduce framework to partition the map outputs based on certain key fields, not the whole keys."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: cannot create directory ‘data2’: File exists\r\n"
     ]
    }
   ],
   "source": [
    "!mkdir data2\n",
    "!echo \"12.12.1.2 11.14.2.3 11.11.4.1 13.12.1.1 14.14.2.2\" > data2/example.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-10-14 20:36:43,029 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false\r\n"
     ]
    }
   ],
   "source": [
    "!hadoop fs -mkdir -p /outputs/partition2\n",
    "!hadoop fs -mkdir -p /inputs/partition2\n",
    "!hadoop fs -put data2 /inputs/partition2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1 items\r\n",
      "-rw-r--r--   3 root supergroup         50 2020-10-14 20:36 /inputs/partition2/data2/example.txt\r\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -ls /inputs/partition2/data2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-10-14 20:36:47,063 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false\n",
      "12.12.1.2 11.14.2.3 11.11.4.1 13.12.1.1 14.14.2.2\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -cat /inputs/partition2/data2/example.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-10-14 20:36:52,927 WARN streaming.StreamJob: -file option is deprecated, please use generic option -files instead.\n",
      "packageJobJar: [/training/paritioning/mapper.py, /training/paritioning/reducer.py, /tmp/hadoop-unjar1619386087736125637/] [] /tmp/streamjob600224126625436347.jar tmpDir=null\n",
      "2020-10-14 20:36:53,596 INFO client.RMProxy: Connecting to ResourceManager at resourcemanager/172.18.0.2:8032\n",
      "2020-10-14 20:36:53,715 INFO client.AHSProxy: Connecting to Application History server at historyserver/172.18.0.6:10200\n",
      "2020-10-14 20:36:53,736 INFO client.RMProxy: Connecting to ResourceManager at resourcemanager/172.18.0.2:8032\n",
      "2020-10-14 20:36:53,736 INFO client.AHSProxy: Connecting to Application History server at historyserver/172.18.0.6:10200\n",
      "2020-10-14 20:36:53,864 INFO mapreduce.JobResourceUploader: Disabling Erasure Coding for path: /tmp/hadoop-yarn/staging/root/.staging/job_1602682693586_0013\n",
      "2020-10-14 20:36:53,990 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false\n",
      "2020-10-14 20:36:54,053 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false\n",
      "2020-10-14 20:36:54,080 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false\n",
      "2020-10-14 20:36:54,133 INFO mapred.FileInputFormat: Total input files to process : 1\n",
      "2020-10-14 20:36:54,148 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false\n",
      "2020-10-14 20:36:54,164 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false\n",
      "2020-10-14 20:36:54,168 INFO mapreduce.JobSubmitter: number of splits:2\n",
      "2020-10-14 20:36:54,191 INFO Configuration.deprecation: mapred.text.key.partitioner.options is deprecated. Instead, use mapreduce.partition.keypartitioner.options\n",
      "2020-10-14 20:36:54,192 INFO Configuration.deprecation: mapred.reduce.tasks is deprecated. Instead, use mapreduce.job.reduces\n",
      "2020-10-14 20:36:54,192 INFO Configuration.deprecation: map.output.key.field.separator is deprecated. Instead, use mapreduce.map.output.key.field.separator\n",
      "2020-10-14 20:36:54,282 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false\n",
      "2020-10-14 20:36:54,294 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1602682693586_0013\n",
      "2020-10-14 20:36:54,294 INFO mapreduce.JobSubmitter: Executing with tokens: []\n",
      "2020-10-14 20:36:54,412 INFO conf.Configuration: resource-types.xml not found\n",
      "2020-10-14 20:36:54,412 INFO resource.ResourceUtils: Unable to find 'resource-types.xml'.\n",
      "2020-10-14 20:36:54,669 INFO impl.YarnClientImpl: Submitted application application_1602682693586_0013\n",
      "2020-10-14 20:36:54,715 INFO mapreduce.Job: The url to track the job: http://resourcemanager:8088/proxy/application_1602682693586_0013/\n",
      "2020-10-14 20:36:54,717 INFO mapreduce.Job: Running job: job_1602682693586_0013\n",
      "2020-10-14 20:36:58,803 INFO mapreduce.Job: Job job_1602682693586_0013 running in uber mode : false\n",
      "2020-10-14 20:36:58,806 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "2020-10-14 20:37:03,904 INFO mapreduce.Job:  map 50% reduce 0%\n",
      "2020-10-14 20:37:04,916 INFO mapreduce.Job:  map 100% reduce 0%\n",
      "2020-10-14 20:37:06,926 INFO mapreduce.Job:  map 100% reduce 25%\n",
      "2020-10-14 20:37:09,961 INFO mapreduce.Job:  map 100% reduce 50%\n",
      "2020-10-14 20:37:11,968 INFO mapreduce.Job:  map 100% reduce 75%\n",
      "2020-10-14 20:37:14,991 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "2020-10-14 20:37:16,012 INFO mapreduce.Job: Job job_1602682693586_0013 completed successfully\n",
      "2020-10-14 20:37:16,062 INFO mapreduce.Job: Counters: 54\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=137\n",
      "\t\tFILE: Number of bytes written=1402134\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=291\n",
      "\t\tHDFS: Number of bytes written=55\n",
      "\t\tHDFS: Number of read operations=26\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=8\n",
      "\t\tHDFS: Number of bytes read erasure-coded=0\n",
      "\tJob Counters \n",
      "\t\tLaunched map tasks=2\n",
      "\t\tLaunched reduce tasks=4\n",
      "\t\tRack-local map tasks=2\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=13532\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=45504\n",
      "\t\tTotal time spent by all map tasks (ms)=3383\n",
      "\t\tTotal time spent by all reduce tasks (ms)=5688\n",
      "\t\tTotal vcore-milliseconds taken by all map tasks=3383\n",
      "\t\tTotal vcore-milliseconds taken by all reduce tasks=5688\n",
      "\t\tTotal megabyte-milliseconds taken by all map tasks=13856768\n",
      "\t\tTotal megabyte-milliseconds taken by all reduce tasks=46596096\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=1\n",
      "\t\tMap output records=5\n",
      "\t\tMap output bytes=55\n",
      "\t\tMap output materialized bytes=161\n",
      "\t\tInput split bytes=216\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=5\n",
      "\t\tReduce shuffle bytes=161\n",
      "\t\tReduce input records=5\n",
      "\t\tReduce output records=5\n",
      "\t\tSpilled Records=10\n",
      "\t\tShuffled Maps =8\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=8\n",
      "\t\tGC time elapsed (ms)=186\n",
      "\t\tCPU time spent (ms)=1980\n",
      "\t\tPhysical memory (bytes) snapshot=1407823872\n",
      "\t\tVirtual memory (bytes) snapshot=43614519296\n",
      "\t\tTotal committed heap usage (bytes)=1518862336\n",
      "\t\tPeak Map Physical memory (bytes)=296656896\n",
      "\t\tPeak Map Virtual memory (bytes)=5039271936\n",
      "\t\tPeak Reduce Physical memory (bytes)=219705344\n",
      "\t\tPeak Reduce Virtual memory (bytes)=8385998848\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=75\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=55\n",
      "2020-10-14 20:37:16,062 INFO streaming.StreamJob: Output directory: /outputs/partition2/sample6\n"
     ]
    }
   ],
   "source": [
    "!hadoop jar /opt/hadoop-3.2.1/share/hadoop/tools/lib/hadoop-streaming-3.2.1.jar \\\n",
    "    -D mapred.reduce.tasks=4 \\\n",
    "    -D stream.map.output.field.separator=. \\\n",
    "    -D stream.num.map.output.key.fields=4 \\\n",
    "    -D map.output.key.field.separator=. \\\n",
    "    -D mapred.text.key.partitioner.options=-k2,3 \\\n",
    "    -partitioner org.apache.hadoop.mapred.lib.KeyFieldBasedPartitioner \\\n",
    "    -file $PWD/mapper.py\\\n",
    "    -file $PWD/reducer.py\\\n",
    "    -mapper mapper.py \\\n",
    "    -reducer reducer.py \\\n",
    "    -input /inputs/partition2/data2 \\\n",
    "    -output /outputs/partition2/sample6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 5 items\r\n",
      "-rw-r--r--   3 root supergroup          0 2020-10-14 20:37 /outputs/partition2/sample6/_SUCCESS\r\n",
      "-rw-r--r--   3 root supergroup         22 2020-10-14 20:37 /outputs/partition2/sample6/part-00000\r\n",
      "-rw-r--r--   3 root supergroup          0 2020-10-14 20:37 /outputs/partition2/sample6/part-00001\r\n",
      "-rw-r--r--   3 root supergroup         11 2020-10-14 20:37 /outputs/partition2/sample6/part-00002\r\n",
      "-rw-r--r--   3 root supergroup         22 2020-10-14 20:37 /outputs/partition2/sample6/part-00003\r\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -ls /outputs/partition2/sample6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-10-14 20:37:24,641 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false\r\n",
      "12.12.1.2\t\r\n",
      "13.12.1.1\t\r\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -cat /outputs/partition2/sample6/part-00000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "!hdfs dfs -cat /outputs/partition2/sample6/part-00001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-10-14 20:37:32,510 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false\n",
      "11.11.4.1\t\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -cat /outputs/partition2/sample6/part-00002"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-10-14 20:37:35,216 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false\n",
      "11.14.2.3\t\n",
      "14.14.2.2\t\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -cat /outputs/partition2/sample6/part-00003"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
