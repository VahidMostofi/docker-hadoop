{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple Example on how to use counters in Hadoop streaming\n",
    "\n",
    "By: Vahid Mostofi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: cannot create directory ‘data’: File exists\r\n"
     ]
    }
   ],
   "source": [
    "!mkdir data\n",
    "!echo \"dog cat cat apple ninja\" > data/text1.txt\n",
    "!echo \"ninja apple cat keyboard horse\" > data/another_file.txt\n",
    "!echo \"water cat dog spider spider\" > data/yet_another_file.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "put: `/inputs/data/another_file.txt': File exists\r\n",
      "put: `/inputs/data/yet_another_file.txt': File exists\r\n",
      "put: `/inputs/data/text1.txt': File exists\r\n"
     ]
    }
   ],
   "source": [
    "!hadoop fs -mkdir -p /outputs\n",
    "!hadoop fs -mkdir -p /inputs\n",
    "!hadoop fs -put data /inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: Use of this script to execute dfs is deprecated.\n",
      "WARNING: Attempting to execute replacement \"hdfs dfs\" instead.\n",
      "\n",
      "Found 3 items\n",
      "-rw-r--r--   3 root supergroup         31 2020-09-30 21:18 /inputs/data/another_file.txt\n",
      "-rw-r--r--   3 root supergroup         24 2020-09-30 21:18 /inputs/data/text1.txt\n",
      "-rw-r--r--   3 root supergroup         28 2020-09-30 21:18 /inputs/data/yet_another_file.txt\n"
     ]
    }
   ],
   "source": [
    "!hadoop dfs -ls /inputs/data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mapper.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mapper.py\n",
    "#!/opt/bitnami/python/bin/python\n",
    "# -*-coding:utf-8 -*\n",
    "import sys\n",
    "\n",
    "for line in sys.stdin: # reads from stdin\n",
    "    line = line.strip()\n",
    "    words = line.split()\n",
    "    # find number of lines\n",
    "    print(\"reporter:counter:%s,%s,%d\" % ('example-info','line_count',1), file=sys.stderr)\n",
    "    for word in words: # writes to stdout\n",
    "        # find sum of the lengths of all words\n",
    "        print(\"reporter:counter:%s,%s,%d\" % ('example-info','len',len(word)), file=sys.stderr)\n",
    "        print(\"%s\\t%d\" % (word, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting reducer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile reducer.py\n",
    "#!/opt/bitnami/python/bin/python\n",
    "# -*-coding:utf-8 -*\n",
    "\n",
    "import sys\n",
    "total = 0\n",
    "lastword = None\n",
    "\n",
    "for line in sys.stdin:\n",
    "    # find number of lines\n",
    "    print(\"reporter:counter:%s,%s,%d\" % ('example-info','line_count',1), file=sys.stderr)\n",
    "    line = line.strip()\n",
    "    \n",
    "    word, count = line.split()\n",
    "    count = int(count)\n",
    "\n",
    "    if lastword is None:\n",
    "        lastword = word\n",
    "    if word == lastword:\n",
    "        total += count\n",
    "    else:\n",
    "        print(\"%s\\t%d occurences\" % (lastword, total))\n",
    "        total = count\n",
    "        lastword = word\n",
    "    \n",
    "if lastword is not None:\n",
    "    print(\"%s\\t%d occurences\" % (lastword, total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted /outputs/out\n",
      "2020-10-21 17:54:33,873 WARN streaming.StreamJob: -file option is deprecated, please use generic option -files instead.\n",
      "packageJobJar: [/training/counters/mapper.py, /training/counters/reducer.py, /tmp/hadoop-unjar1789546134438123751/] [] /tmp/streamjob3943389933522019500.jar tmpDir=null\n",
      "2020-10-21 17:54:34,668 INFO client.RMProxy: Connecting to ResourceManager at resourcemanager/172.18.0.6:8032\n",
      "2020-10-21 17:54:34,826 INFO client.AHSProxy: Connecting to Application History server at historyserver/172.18.0.4:10200\n",
      "2020-10-21 17:54:34,848 INFO client.RMProxy: Connecting to ResourceManager at resourcemanager/172.18.0.6:8032\n",
      "2020-10-21 17:54:34,849 INFO client.AHSProxy: Connecting to Application History server at historyserver/172.18.0.4:10200\n",
      "2020-10-21 17:54:34,983 INFO mapreduce.JobResourceUploader: Disabling Erasure Coding for path: /tmp/hadoop-yarn/staging/root/.staging/job_1603141762614_0004\n",
      "2020-10-21 17:54:35,087 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false\n",
      "2020-10-21 17:54:35,159 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false\n",
      "2020-10-21 17:54:35,182 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false\n",
      "2020-10-21 17:54:35,252 INFO mapred.FileInputFormat: Total input files to process : 3\n",
      "2020-10-21 17:54:35,272 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false\n",
      "2020-10-21 17:54:35,294 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false\n",
      "2020-10-21 17:54:35,314 INFO mapreduce.JobSubmitter: number of splits:3\n",
      "2020-10-21 17:54:35,396 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false\n",
      "2020-10-21 17:54:35,408 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1603141762614_0004\n",
      "2020-10-21 17:54:35,408 INFO mapreduce.JobSubmitter: Executing with tokens: []\n",
      "2020-10-21 17:54:35,583 INFO conf.Configuration: resource-types.xml not found\n",
      "2020-10-21 17:54:35,583 INFO resource.ResourceUtils: Unable to find 'resource-types.xml'.\n",
      "2020-10-21 17:54:36,245 INFO impl.YarnClientImpl: Submitted application application_1603141762614_0004\n",
      "2020-10-21 17:54:36,337 INFO mapreduce.Job: The url to track the job: http://resourcemanager:8088/proxy/application_1603141762614_0004/\n",
      "2020-10-21 17:54:36,339 INFO mapreduce.Job: Running job: job_1603141762614_0004\n",
      "2020-10-21 17:54:41,500 INFO mapreduce.Job: Job job_1603141762614_0004 running in uber mode : false\n",
      "2020-10-21 17:54:41,502 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "2020-10-21 17:54:46,617 INFO mapreduce.Job:  map 33% reduce 0%\n",
      "2020-10-21 17:54:47,633 INFO mapreduce.Job:  map 67% reduce 0%\n",
      "2020-10-21 17:54:48,646 INFO mapreduce.Job:  map 100% reduce 0%\n",
      "2020-10-21 17:54:50,664 INFO mapreduce.Job:  map 100% reduce 50%\n",
      "2020-10-21 17:54:53,695 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "2020-10-21 17:54:54,734 INFO mapreduce.Job: Job job_1603141762614_0004 completed successfully\n",
      "2020-10-21 17:54:54,888 INFO mapreduce.Job: Counters: 56\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=120\n",
      "\t\tFILE: Number of bytes written=1164347\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=383\n",
      "\t\tHDFS: Number of bytes written=152\n",
      "\t\tHDFS: Number of read operations=19\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=4\n",
      "\t\tHDFS: Number of bytes read erasure-coded=0\n",
      "\tJob Counters \n",
      "\t\tLaunched map tasks=3\n",
      "\t\tLaunched reduce tasks=2\n",
      "\t\tRack-local map tasks=3\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=24556\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=25432\n",
      "\t\tTotal time spent by all map tasks (ms)=6139\n",
      "\t\tTotal time spent by all reduce tasks (ms)=3179\n",
      "\t\tTotal vcore-milliseconds taken by all map tasks=6139\n",
      "\t\tTotal vcore-milliseconds taken by all reduce tasks=3179\n",
      "\t\tTotal megabyte-milliseconds taken by all map tasks=25145344\n",
      "\t\tTotal megabyte-milliseconds taken by all reduce tasks=26042368\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=3\n",
      "\t\tMap output records=15\n",
      "\t\tMap output bytes=113\n",
      "\t\tMap output materialized bytes=205\n",
      "\t\tInput split bytes=300\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=8\n",
      "\t\tReduce shuffle bytes=205\n",
      "\t\tReduce input records=15\n",
      "\t\tReduce output records=8\n",
      "\t\tSpilled Records=30\n",
      "\t\tShuffled Maps =6\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=6\n",
      "\t\tGC time elapsed (ms)=220\n",
      "\t\tCPU time spent (ms)=1870\n",
      "\t\tPhysical memory (bytes) snapshot=1321316352\n",
      "\t\tVirtual memory (bytes) snapshot=31885111296\n",
      "\t\tTotal committed heap usage (bytes)=1439170560\n",
      "\t\tPeak Map Physical memory (bytes)=309641216\n",
      "\t\tPeak Map Virtual memory (bytes)=5039489024\n",
      "\t\tPeak Reduce Physical memory (bytes)=220336128\n",
      "\t\tPeak Reduce Virtual memory (bytes)=8385454080\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\texample-info\n",
      "\t\tlen=68\n",
      "\t\tline_count=18\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=83\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=152\n",
      "2020-10-21 17:54:54,888 INFO streaming.StreamJob: Output directory: /outputs/out\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -rm -R /outputs/out\n",
    "!hadoop jar /opt/hadoop-3.2.1/share/hadoop/tools/lib/hadoop-streaming-3.2.1.jar \\\n",
    "    -D mapreduce.job.reduces=2 \\\n",
    "    -file $PWD/mapper.py \\\n",
    "    -file $PWD/reducer.py \\\n",
    "    -mapper mapper.py \\\n",
    "    -reducer reducer.py \\\n",
    "    -input /inputs/data \\\n",
    "    -output /outputs/out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we can see the counter values in the output of Map-Reduce Job\n",
    "\n",
    "example-info ...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
