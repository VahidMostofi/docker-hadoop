{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple WordCount Example for MapReduce using Hadoop Streaming and Python\n",
    "\n",
    "By: Vahid Mostofi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ceate some input files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir data\n",
    "!echo \"dog cat cat apple ninja\" > data/text1.txt\n",
    "!echo \"ninja apple cat keyboard horse\" > data/another_file.txt\n",
    "!echo \"water cat dog spider spider\" > data/yet_another_file.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### move input files to hdfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "put: `/inputs/data/text1.txt': File exists\r\n",
      "put: `/inputs/data/another_file.txt': File exists\r\n",
      "put: `/inputs/data/yet_another_file.txt': File exists\r\n"
     ]
    }
   ],
   "source": [
    "!hadoop fs -mkdir -p /outputs\n",
    "!hadoop fs -mkdir -p /inputs\n",
    "!hadoop fs -put data /inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### list the content of the HDFS folder we just created\n",
    "The three files we created are now stored on the HDFS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: Use of this script to execute dfs is deprecated.\n",
      "WARNING: Attempting to execute replacement \"hdfs dfs\" instead.\n",
      "\n",
      "Found 3 items\n",
      "-rw-r--r--   3 root supergroup         31 2020-08-25 20:58 /inputs/data/another_file.txt\n",
      "-rw-r--r--   3 root supergroup         24 2020-08-25 20:58 /inputs/data/text1.txt\n",
      "-rw-r--r--   3 root supergroup         28 2020-08-25 20:58 /inputs/data/yet_another_file.txt\n"
     ]
    }
   ],
   "source": [
    "!hadoop dfs -ls /inputs/data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### mapper\n",
    "this is the word count example, so we need to create the mapper and reducer.\n",
    "\n",
    " * the ```%%writefile mapper.py``` tells Jupyter to save the content of the cell as a file named mapper.py in the same direcotry. So the #!/opt/bit.... is the first line of the file.\n",
    " * the ```#!/opt/bitnami/python/bin/python``` specifies the Python path for running the file.\n",
    " * the mapper reads from the stdin, which is provided to it by the MapReduce framework and also writes to stdout (using print)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mapper.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mapper.py\n",
    "#!/opt/bitnami/python/bin/python\n",
    "# -*-coding:utf-8 -*\n",
    "import sys\n",
    "for line in sys.stdin: # reads from stdin\n",
    "    print(\"your message B\", file=sys.stderr)\n",
    "    line = line.strip()\n",
    "    words = line.split()\n",
    "\n",
    "    for word in words: # writes to stdout\n",
    "        print(\"%s\\t%d\" % (word, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### reducer\n",
    "similarly to previous cell, here we create a file, named ```reducer.py``` and store the logic for our reducer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting reducer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile reducer.py\n",
    "#!/opt/bitnami/python/bin/python\n",
    "# -*-coding:utf-8 -*\n",
    "\n",
    "import sys\n",
    "total = 0\n",
    "lastword = None\n",
    "\n",
    "for line in sys.stdin:\n",
    "    line = line.strip()\n",
    "\n",
    "    # recuperer la cle et la valeur et conversion de la valeur en int\n",
    "    word, count = line.split()\n",
    "    count = int(count)\n",
    "\n",
    "    # passage au mot suivant (plusieurs cles possibles pour une même exécution de programme)\n",
    "    if lastword is None:\n",
    "        lastword = word\n",
    "    if word == lastword:\n",
    "        total += count\n",
    "    else:\n",
    "        print(\"%s\\t%d occurences\" % (lastword, total))\n",
    "        total = count\n",
    "        lastword = word\n",
    "\n",
    "if lastword is not None:\n",
    "    print(\"%s\\t%d occurences\" % (lastword, total))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### run MapReduce\n",
    "now to we need to run the map reduce program using our mapper.py and reducer.py\n",
    "\n",
    " * ```!hadoop jar /opt/hadoop-3.2.1/share/hadoop/tools/lib/hadoop-streaming-3.2.1.jar ``` specifies the path for streaming module of MapReduce\n",
    " * ```-file``` tells map-reduce which files should be moved to the worker nodes, you can also move txt files and read them in your mapper.py or reducer.py\n",
    " * ```-mapper``` and ```-reducer``` specify the the commands for mapper and reducer, because mapper.py file has the path to python as the first line, the system would know how to execute it.\n",
    " * ```-input``` tells which folder should be scanned for input, all the files in this folder would be fed to the mappers (mapper.py) as stdin\n",
    " * ```-output``` specifies the path for the folder which the output of the map-reduce execution should be stored. Remmeber the folder must be empty, in other words every single execution of the following command needs a new folder. You don't need to crate the folder before hand, just make sure you use a new path each time.\n",
    " \n",
    " * for more infomration about map-reduce streaming API please use  http://hadoop.apache.org/docs/r1.2.1/streaming.html\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-08-25 21:33:39,826 WARN streaming.StreamJob: -file option is deprecated, please use generic option -files instead.\n",
      "packageJobJar: [/training/mapper.py, /training/reducer.py, /tmp/hadoop-unjar5501641474586690589/] [] /tmp/streamjob7630392549135838828.jar tmpDir=null\n",
      "2020-08-25 21:33:41,205 INFO client.RMProxy: Connecting to ResourceManager at resourcemanager/172.19.0.4:8032\n",
      "2020-08-25 21:33:41,489 INFO client.AHSProxy: Connecting to Application History server at historyserver/172.19.0.2:10200\n",
      "2020-08-25 21:33:41,525 INFO client.RMProxy: Connecting to ResourceManager at resourcemanager/172.19.0.4:8032\n",
      "2020-08-25 21:33:41,526 INFO client.AHSProxy: Connecting to Application History server at historyserver/172.19.0.2:10200\n",
      "2020-08-25 21:33:41,858 INFO mapreduce.JobResourceUploader: Disabling Erasure Coding for path: /tmp/hadoop-yarn/staging/root/.staging/job_1598390533934_0002\n",
      "2020-08-25 21:33:42,058 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false\n",
      "2020-08-25 21:33:42,638 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false\n",
      "2020-08-25 21:33:42,725 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false\n",
      "2020-08-25 21:33:42,909 INFO mapred.FileInputFormat: Total input files to process : 3\n",
      "2020-08-25 21:33:42,946 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false\n",
      "2020-08-25 21:33:43,400 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false\n",
      "2020-08-25 21:33:43,433 INFO mapreduce.JobSubmitter: number of splits:3\n",
      "2020-08-25 21:33:43,619 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false\n",
      "2020-08-25 21:33:43,711 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1598390533934_0002\n",
      "2020-08-25 21:33:43,711 INFO mapreduce.JobSubmitter: Executing with tokens: []\n",
      "2020-08-25 21:33:44,142 INFO conf.Configuration: resource-types.xml not found\n",
      "2020-08-25 21:33:44,143 INFO resource.ResourceUtils: Unable to find 'resource-types.xml'.\n",
      "2020-08-25 21:33:44,880 INFO impl.YarnClientImpl: Submitted application application_1598390533934_0002\n",
      "2020-08-25 21:33:44,926 INFO mapreduce.Job: The url to track the job: http://resourcemanager:8088/proxy/application_1598390533934_0002/\n",
      "2020-08-25 21:33:44,930 INFO mapreduce.Job: Running job: job_1598390533934_0002\n",
      "2020-08-25 21:33:55,300 INFO mapreduce.Job: Job job_1598390533934_0002 running in uber mode : false\n",
      "2020-08-25 21:33:55,304 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "2020-08-25 21:34:05,462 INFO mapreduce.Job:  map 33% reduce 0%\n",
      "2020-08-25 21:34:06,478 INFO mapreduce.Job:  map 67% reduce 0%\n",
      "2020-08-25 21:34:07,491 INFO mapreduce.Job:  map 100% reduce 0%\n",
      "2020-08-25 21:34:12,576 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "2020-08-25 21:34:13,607 INFO mapreduce.Job: Job job_1598390533934_0002 completed successfully\n",
      "2020-08-25 21:34:13,782 INFO mapreduce.Job: Counters: 54\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=92\n",
      "\t\tFILE: Number of bytes written=931390\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=383\n",
      "\t\tHDFS: Number of bytes written=152\n",
      "\t\tHDFS: Number of read operations=14\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=2\n",
      "\t\tHDFS: Number of bytes read erasure-coded=0\n",
      "\tJob Counters \n",
      "\t\tLaunched map tasks=3\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tRack-local map tasks=3\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=80756\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=31296\n",
      "\t\tTotal time spent by all map tasks (ms)=20189\n",
      "\t\tTotal time spent by all reduce tasks (ms)=3912\n",
      "\t\tTotal vcore-milliseconds taken by all map tasks=20189\n",
      "\t\tTotal vcore-milliseconds taken by all reduce tasks=3912\n",
      "\t\tTotal megabyte-milliseconds taken by all map tasks=82694144\n",
      "\t\tTotal megabyte-milliseconds taken by all reduce tasks=32047104\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=3\n",
      "\t\tMap output records=15\n",
      "\t\tMap output bytes=113\n",
      "\t\tMap output materialized bytes=157\n",
      "\t\tInput split bytes=300\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=8\n",
      "\t\tReduce shuffle bytes=157\n",
      "\t\tReduce input records=15\n",
      "\t\tReduce output records=8\n",
      "\t\tSpilled Records=30\n",
      "\t\tShuffled Maps =3\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=3\n",
      "\t\tGC time elapsed (ms)=688\n",
      "\t\tCPU time spent (ms)=3880\n",
      "\t\tPhysical memory (bytes) snapshot=1070694400\n",
      "\t\tVirtual memory (bytes) snapshot=23518568448\n",
      "\t\tTotal committed heap usage (bytes)=993525760\n",
      "\t\tPeak Map Physical memory (bytes)=298950656\n",
      "\t\tPeak Map Virtual memory (bytes)=5044219904\n",
      "\t\tPeak Reduce Physical memory (bytes)=204668928\n",
      "\t\tPeak Reduce Virtual memory (bytes)=8388845568\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=83\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=152\n",
      "2020-08-25 21:34:13,783 INFO streaming.StreamJob: Output directory: /outputs/result2\n"
     ]
    }
   ],
   "source": [
    "!hadoop jar /opt/hadoop-3.2.1/share/hadoop/tools/lib/hadoop-streaming-3.2.1.jar \\\n",
    "    -file $PWD/mapper.py\\\n",
    "    -file $PWD/reducer.py\\\n",
    "    -mapper mapper.py \\\n",
    "    -reducer reducer.py \\\n",
    "    -input /inputs/data \\\n",
    "    -output /outputs/result2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### lets look at the output of the map reduce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2 items\r\n",
      "-rw-r--r--   3 root supergroup          0 2020-08-25 21:34 /outputs/result2/_SUCCESS\r\n",
      "-rw-r--r--   3 root supergroup        152 2020-08-25 21:34 /outputs/result2/part-00000\r\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -ls /outputs/result2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-08-25 21:36:33,512 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false\n",
      "apple\t2 occurences\n",
      "cat\t4 occurences\n",
      "dog\t2 occurences\n",
      "horse\t1 occurences\n",
      "keyboard\t1 occurences\n",
      "ninja\t2 occurences\n",
      "spider\t2 occurences\n",
      "water\t1 occurences\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -cat /outputs/result2/part-00000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### make a directory out of HDFS and store the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-08-25 21:38:54,446 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false\r\n"
     ]
    }
   ],
   "source": [
    "!mkdir -p /outputs/res_out_of_hdfs\n",
    "!hdfs dfs -copyToLocal /outputs/result2/* /outputs/res_out_of_hdfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_SUCCESS  part-00000\r\n"
     ]
    }
   ],
   "source": [
    "!ls /outputs/res_out_of_hdfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "apple\t2 occurences\r\n",
      "cat\t4 occurences\r\n",
      "dog\t2 occurences\r\n",
      "horse\t1 occurences\r\n",
      "keyboard\t1 occurences\r\n",
      "ninja\t2 occurences\r\n",
      "spider\t2 occurences\r\n",
      "water\t1 occurences\r\n"
     ]
    }
   ],
   "source": [
    "!cat /outputs/res_out_of_hdfs/part-00000"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
